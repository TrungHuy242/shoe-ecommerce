"""
Footy AI Assistant v2.0 - Intelligent Shopping Assistant for FootFashion
N√¢ng c·∫•p v·ªõi Gemini Pro v√† context-aware product recommendations
"""

import os
import re
import time
from typing import Dict, List, Any, Optional, Tuple
from collections import deque
import google.generativeai as genai
from django.conf import settings
from django.db.models import Q
from django.utils import timezone
from django.core.cache import cache
from difflib import SequenceMatcher
import logging

# Import Django models
from ..models import Product, Brand, Category, Gender, Size, Color, Promotion, Order

logger = logging.getLogger(__name__)

# Configure Gemini API
GEMINI_API_KEY = getattr(settings, 'GEMINI_API_KEY', None) or os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)


class ConversationMemory:
    """Qu·∫£n l√Ω ng·ªØ c·∫£nh h·ªôi tho·∫°i trong 5 l∆∞·ª£t g·∫ßn nh·∫•t"""
    
    def __init__(self, max_size: int = 5):
        self.memories = {}  # user_id -> deque of conversations
        self.max_size = max_size
    
    def add_conversation(self, user_id: str, message: str, response: str, intent: str):
        """Th√™m cu·ªôc h·ªôi tho·∫°i v√†o memory"""
        if user_id not in self.memories:
            self.memories[user_id] = deque(maxlen=self.max_size)
        
        conversation = {
            'message': message,
            'response': response,
            'intent': intent,
            'timestamp': timezone.now().isoformat()
        }
        self.memories[user_id].append(conversation)
    
    def get_context(self, user_id: str) -> List[Dict]:
        """L·∫•y ng·ªØ c·∫£nh h·ªôi tho·∫°i g·∫ßn nh·∫•t"""
        return list(self.memories.get(user_id, []))
    
    def clear_context(self, user_id: str):
        """X√≥a ng·ªØ c·∫£nh c·ªßa user"""
        if user_id in self.memories:
            del self.memories[user_id]


class ProductContextBuilder:
    """X√¢y d·ª±ng context s·∫£n ph·∫©m t·ª´ database"""
    
    @staticmethod
    def get_products_context(limit: int = 50) -> str:
        """L·∫•y context s·∫£n ph·∫©m t·ª´ database"""
        try:
            # L·∫•y s·∫£n ph·∫©m v·ªõi th√¥ng tin ƒë·∫ßy ƒë·ªß
            products = Product.objects.select_related('brand', 'category', 'gender').prefetch_related(
                'sizes', 'colors', 'images'
            ).order_by('-id')[:limit]
            
            if not products.exists():
                return "Hi·ªán t·∫°i c·ª≠a h√†ng ch∆∞a c√≥ s·∫£n ph·∫©m n√†o."
            
            context_lines = []
            for product in products:
                # T·∫°o m√¥ t·∫£ ng·∫Øn g·ªçn cho t·ª´ng s·∫£n ph·∫©m
                description_parts = []
                
                # T√™n v√† th∆∞∆°ng hi·ªáu
                description_parts.append(f"{product.name} - {product.brand.name}")
                
                # Gi√°
                price_formatted = f"{product.price:,.0f} VND"
                description_parts.append(f"Gi√°: {price_formatted}")
                
                # M√¥ t·∫£ ng·∫Øn
                if product.description:
                    # L·∫•y 100 k√Ω t·ª± ƒë·∫ßu c·ªßa m√¥ t·∫£
                    short_desc = product.description[:100].strip()
                    if len(product.description) > 100:
                        short_desc += "..."
                    description_parts.append(f"M√¥ t·∫£: {short_desc}")
                
                # Gi·ªõi t√≠nh
                if product.gender:
                    description_parts.append(f"Gi·ªõi t√≠nh: {product.gender.name}")
                
                # Sizes c√≥ s·∫µn
                sizes = product.sizes.all()[:5]  # L·∫•y t·ªëi ƒëa 5 size
                if sizes:
                    size_values = [str(size.value) for size in sizes]
                    description_parts.append(f"Sizes: {', '.join(size_values)}")
                
                # M√†u s·∫Øc c√≥ s·∫µn
                colors = product.colors.all()[:3]  # L·∫•y t·ªëi ƒëa 3 m√†u
                if colors:
                    color_values = [color.value for color in colors]
                    description_parts.append(f"M√†u s·∫Øc: {', '.join(color_values)}")
                
                # S·ªë l∆∞·ª£ng b√°n ƒë∆∞·ª£c
                if product.sales_count > 0:
                    description_parts.append(f"ƒê√£ b√°n: {product.sales_count} ƒë√¥i")
                
                # Gh√©p th√†nh m·ªôt d√≤ng
                product_line = " | ".join(description_parts)
                context_lines.append(f"- {product_line}")
            
            return "\n".join(context_lines)
            
        except Exception as e:
            logger.error(f"Error building product context: {e}")
            return "Kh√¥ng th·ªÉ t·∫£i th√¥ng tin s·∫£n ph·∫©m."
    
    @staticmethod
    def get_promotions_context() -> str:
        """L·∫•y context khuy·∫øn m√£i"""
        try:
            now = timezone.now()
            promotions = Promotion.objects.filter(
                is_active=True,
                start_date__lte=now,
                end_date__gte=now
            ).order_by('-discount_percentage')[:10]
            
            if not promotions.exists():
                return "Hi·ªán t·∫°i kh√¥ng c√≥ khuy·∫øn m√£i n√†o."
            
            context_lines = []
            for promo in promotions:
                end_date_str = promo.end_date.strftime('%d/%m/%Y') if promo.end_date else "Kh√¥ng gi·ªõi h·∫°n"
                context_lines.append(
                    f"- M√£: {promo.code} | Gi·∫£m {promo.discount_percentage}% | H·∫øt h·∫°n: {end_date_str}"
                )
            
            return "\n".join(context_lines)
            
        except Exception as e:
            logger.error(f"Error building promotions context: {e}")
            return "Kh√¥ng th·ªÉ t·∫£i th√¥ng tin khuy·∫øn m√£i."


class AdvancedNLPProcessor:
    """X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n n√¢ng cao"""
    
    def __init__(self):
        # T·ª´ ƒë·ªìng nghƒ©a v√† bi·∫øn th·ªÉ - Enhanced for natural language
        self.synonyms = {
            'gi√†y': ['giay', 'gi√†y d√©p', 'shoe', 'shoes', 'sneaker', 'boot', 'ƒë√¥i gi√†y', 'c√°i gi√†y'],
            'd√©p': ['dep', 'sandals', 'flip-flop', 'slipper', 'ƒë√¥i d√©p', 'c√°i d√©p'],
            'nike': ['nike', 'nike air', 'air max', 'jordan'],
            'adidas': ['adidas', 'adidas originals', 'ultraboost'],
            'puma': ['puma', 'puma suede'],
            'vans': ['vans', 'vans old skool'],
            'converse': ['converse', 'chuck taylor', 'all star'],
            'nam': ['nam', 'male', 'men', 'ƒë√†n √¥ng', 'con trai'],
            'n·ªØ': ['n·ªØ', 'nu', 'female', 'women', 'ph·ª• n·ªØ', 'con g√°i'],
            'gi√°': ['gia', 'price', 'cost', 'ti·ªÅn', 'gi√° c·∫£'],
            'r·∫ª': ['re', 'cheap', 'affordable', 'gi√° r·∫ª', 'h·ª£p l√Ω'],
            'ƒë·∫Øt': ['dat', 'expensive', 'gi√° cao', 'm·∫Øc'],
            'size': ['s·ªë', 'k√≠ch c·ª°', 'size', 'c·ª°', 's·ªë gi√†y'],
            'm√†u': ['mau', 'color', 'colour', 'm√†u s·∫Øc'],
            'ƒëen': ['den', 'black'],
            'tr·∫Øng': ['trang', 'white'],
            'ƒë·ªè': ['do', 'red'],
            'xanh': ['blue', 'green'],
            't√¨m': ['tim', 'find', 'search', 'look for', 't√¨m ki·∫øm'],
            'mua': ['buy', 'purchase', 'order', 'mua s·∫Øm'],
            'xem': ['see', 'view', 'look at', 'nh√¨n', 'coi'],
            'g·ª£i √Ω': ['goi y', 'suggest', 'recommend', 'advise', 'ƒë·ªÅ xu·∫•t'],
            'khuy·∫øn m√£i': ['khuyen mai', 'promotion', 'sale', 'discount', '∆∞u ƒë√£i'],
            'gi·∫£m gi√°': ['giam gia', 'discount', 'off', 'sale', 'gi·∫£m'],
            'ƒë∆°n h√†ng': ['don hang', 'order', 'orders', 'ƒë∆°n'],
            'c·ªßa t√¥i': ['cua toi', 'my', 'mine', 'c·ªßa m√¨nh'],
            'g·∫ßn ƒë√¢y': ['gan day', 'recent', 'lately', 'm·ªõi ƒë√¢y'],
            't·ªët': ['tot', 'good', 'nice', 'great', 'ƒë·∫πp', 'hay'],
            'ƒë·∫πp': ['dep', 'beautiful', 'pretty', 'nice', 't·ªët'],
            'hay': ['good', 'great', 'awesome', 't·ªët', 'ƒë·∫πp'],
            'c·∫ßn': ['need', 'want', 'require', 'mu·ªën'],
            'mu·ªën': ['want', 'need', 'desire', 'c·∫ßn'],
            'cho t√¥i': ['cho toi', 'give me', 'show me', 't√¥i mu·ªën'],
            'v√†i': ['some', 'a few', 'm·ªôt v√†i', 'm·ªôt s·ªë'],
            'm·ªôt s·ªë': ['some', 'several', 'v√†i', 'm·ªôt v√†i'],
            'h√¥m nay': ['hom nay', 'today', 'hi·ªán t·∫°i', 'b√¢y gi·ªù']
        }
        
        # T·ª´ kh√≥a c·∫£m x√∫c ti·∫øng Vi·ªát - Enhanced for Gen Z
        self.positive_words = [
            't·ªët', 'ƒë·∫πp', 'hay', 'tuy·ªát', 'ok', 'ƒë∆∞·ª£c', 'th√≠ch', 'y√™u', 'h√†i l√≤ng',
            'vui', 'h·∫°nh ph√∫c', 'th√∫ v·ªã', 'th√≠ch th√∫', 'c·∫£m ∆°n', 'thanks', 'perfect',
            'awesome', 'great', 'excellent', 'amazing', 'wonderful',
            'hot', 'hit', 'ngon', 'ch·∫•t', 'x·ªãn', 'ƒë·ªânh', 'pro', 'cool', 'nice',
            '·ªïn', 'ok', 'ƒë∆∞·ª£c', 'tuy·ªát v·ªùi', 'xu·∫•t s·∫Øc', 'tuy·ªát h·∫£o'
        ]
        self.negative_words = [
            't·ªá', 'x·∫•u', 'd·ªü', 'kh√¥ng th√≠ch', 'gh√©t', 'b·ª±c', 't·ª©c', 'kh√≥ ch·ªãu',
            'kh√¥ng h√†i l√≤ng', 'th·∫•t v·ªçng', 'bu·ªìn', 'lo l·∫Øng', 'kh√¥ng ok', 'bad',
            'terrible', 'awful', 'horrible', 'disappointed', 'angry',
            'ƒë·∫Øt', 'm·∫Øc', 'gi√° cao', 's·ª£ mau d∆°', 'kh√¥ng b·ªÅn', 'kh√¥ng √™m',
            'kh√≥ ch·ªãu', 'kh√¥ng ph√π h·ª£p', 'kh√¥ng v·ª´a', 'ch·∫≠t', 'r·ªông',
            'kh√¥ng c√≥', 'h·∫øt h√†ng', 'out of stock', 'sold out'
        ]
        self.urgent_words = [
            'g·∫•p', 'kh·∫©n c·∫•p', 'nhanh', 'ngay', 'l·∫≠p t·ª©c', 'urgent', 'asap',
            'immediately', 'quickly', 'fast', 'hurry'
        ]
        
        # Intent patterns v·ªõi confidence scores - Improved for natural language
        self.intent_patterns = {
            'greeting': [
                (r'\b(xin ch√†o|ch√†o|hello|hi|hey|xin ch[a√†]o)\b', 0.9),
                (r'\b(c√≥ ai|ai ƒë√≥|c√≥ ng∆∞·ªùi)\b', 0.8),
                (r'\b(start|b·∫Øt ƒë·∫ßu|begin)\b', 0.7),
                (r'\b(morning|afternoon|evening)\b', 0.6)
            ],
            'product_search': [
                # Natural language patterns
                (r'\b(t√¥i mu·ªën|t√¥i c·∫ßn|cho t√¥i|t√¨m|mua|xem|c√≥)\b.*\b(gi√†y|d√©p|sneaker|boot|sandal)\b', 0.9),
                (r'\b(gi√†y|d√©p|sneaker|boot|sandal)\b.*\b(nike|adidas|puma|vans|converse)\b', 0.9),
                (r'\b(nike|adidas|puma|vans|converse)\b', 0.8),
                (r'\b(nam|n·ªØ|unisex)\b.*\b(gi√†y|d√©p)\b', 0.8),
                (r'\b(size|m√†u|color|gi√°|price|r·∫ª|ƒë·∫Øt)\b', 0.7),
                (r'\b(d∆∞·ªõi|tr√™n|kho·∫£ng)\b.*\b(tri·ªáu|tr|k|vnd)\b', 0.8),
                (r'\b(gi√†y n√†o|d√©p n√†o|c√≥.*gi√†y|c√≥.*d√©p)\b', 0.7),
                (r'\b(ch·∫°y b·ªô|th·ªÉ thao|c√¥ng s·ªü|ƒëi l√†m|ƒëi ch∆°i|ƒëi h·ªçc)\b', 0.6),
                (r'\b(m·ªôt ƒë√¥i|v√†i ƒë√¥i|v√†i c√°i)\b.*\b(gi√†y|d√©p)\b', 0.6),
                (r'\b(√™m ch√¢n|nh·∫π ch√¢n|tho·∫£i m√°i|comfortable)\b', 0.6),
                (r'\b(ƒë·∫πp|t·ªët|hay|nice|good)\b.*\b(gi√†y|d√©p)\b', 0.6)
            ],
            'recommendation': [
                (r'\b(g·ª£i √Ω|ƒë·ªÅ xu·∫•t|recommend|suggest|n√™n mua|b√°n ch·∫°y|hot|trending)\b', 0.9),
                (r'\b(gi√†y n√†o|d√©p n√†o|s·∫£n ph·∫©m n√†o)\b.*\b(t·ªët|ƒë·∫πp|ch·∫•t l∆∞·ª£ng|b·ªÅn|hay)\b', 0.8),
                (r'\b(top|best|t·ªët nh·∫•t|hay nh·∫•t|ƒë·∫πp nh·∫•t)\b', 0.8),
                (r'\b(ph√π h·ª£p|h·ª£p v·ªõi)\b', 0.7),
                (r'\b(popular|trending|favorite)\b', 0.7),
                (r'\b(g·ª£i √Ω cho|suggest for|recommend for)\b', 0.8),
                (r'\b(c√≥ g·ª£i √Ω|c√≥ suggest|c√≥ recommend)\b', 0.8),
                (r'\b(ƒëi l√†m|ƒëi h·ªçc|c√¥ng s·ªü|th·ªÉ thao)\b.*\b(g·ª£i √Ω|suggest)\b', 0.7)
            ],
            'promotion': [
                (r'\b(khuy·∫øn m√£i|gi·∫£m gi√°|sale|discount|voucher|m√£ gi·∫£m|coupon|∆∞u ƒë√£i)\b', 0.9),
                (r'\b(m√£|code|promo)\b', 0.8),
                (r'\b(gi·∫£m|discount|off)\b', 0.7),
                (r'\b(deal|offer|special)\b', 0.6),
                (r'\b(xem.*khuy·∫øn m√£i|xem.*sale|c√≥.*khuy·∫øn m√£i|c√≥.*sale)\b', 0.8),
                (r'\b(h√¥m nay|today|hi·ªán t·∫°i|now)\b.*\b(khuy·∫øn m√£i|sale|gi·∫£m gi√°)\b', 0.7),
                (r'\b(c√≥.*khuy·∫øn m√£i|c√≥.*sale|c√≥.*gi·∫£m gi√°)\b', 0.8),
                (r'\b(khuy·∫øn m√£i.*h√¥m nay|sale.*h√¥m nay|gi·∫£m gi√°.*h√¥m nay)\b', 0.8)
            ],
            'order_status': [
                (r'\b(ƒë∆°n h√†ng|order|giao h√†ng|v·∫≠n chuy·ªÉn|ship|tracking|theo d√µi)\b', 0.9),
                (r'\b(tr·∫°ng th√°i|t√¨nh tr·∫°ng|status)\b', 0.8),
                (r'\b(khi n√†o|bao gi·ªù|when)\b.*\b(giao|deliver)\b', 0.8),
                (r'\b(tracking|shipment|delivery)\b', 0.7),
                (r'\b(xem.*ƒë∆°n h√†ng|xem.*order|c·ªßa t√¥i|c·ªßa m√¨nh)\b', 0.8),
                (r'\b(g·∫ßn ƒë√¢y|recent|m·ªõi nh·∫•t|latest)\b.*\b(ƒë∆°n h√†ng|order)\b', 0.7)
            ],
            'help': [
                (r'\b(gi√∫p|help|h∆∞·ªõng d·∫´n|h·ªó tr·ª£|support|tr·ª£ gi√∫p)\b', 0.9),
                (r'\b(l√†m sao|how|nh∆∞ th·∫ø n√†o)\b', 0.8),
                (r'\b(help|h·ªó tr·ª£|guide)\b', 0.7),
                (r'\b(tutorial|instruction|manual)\b', 0.6),
                (r'\b(kh√¥ng bi·∫øt|confused|b·ªëi r·ªëi)\b', 0.7)
            ],
            'order_change_request': [
                (r'\b(ƒë·ªïi|thay ƒë·ªïi|change|swap)\b.*\b(size|m√†u|color|ƒë∆°n h√†ng|order)\b', 0.9),
                (r'\b(ƒë·∫∑t|order)\b.*\b(h√¥m qua|yesterday|tr∆∞·ªõc)\b.*\b(ƒë·ªïi|change|thay)\b', 0.9),
                (r'\b(mu·ªën ƒë·ªïi|want to change|thay ƒë·ªïi)\b', 0.8),
                (r'\b(ƒë·ªïi sang|change to|switch to)\b', 0.8),
                (r'\b(size|m√†u|color)\b.*\b(kh√°c|different|other)\b', 0.7)
            ]
        }
    
    def normalize_text(self, text: str) -> str:
        """Chu·∫©n h√≥a text: lowercase, remove accents, expand synonyms"""
        text = text.lower().strip()
        
        # Expand synonyms
        for key, synonyms in self.synonyms.items():
            for synonym in synonyms:
                if synonym in text:
                    text = text.replace(synonym, key)
        
        return text
    
    def fuzzy_match(self, text: str, patterns: List[Tuple[str, float]]) -> Tuple[str, float]:
        """Fuzzy matching v·ªõi confidence score"""
        best_intent = 'unknown'
        best_score = 0.0
        
        normalized_text = self.normalize_text(text)
        
        for intent, pattern_list in self.intent_patterns.items():
            for pattern, base_confidence in pattern_list:
                if re.search(pattern, normalized_text):
                    # Calculate fuzzy score
                    match = re.search(pattern, normalized_text)
                    if match:
                        matched_text = match.group()
                        similarity = SequenceMatcher(None, matched_text, normalized_text).ratio()
                        final_score = base_confidence * similarity
                        
                        if final_score > best_score:
                            best_score = final_score
                            best_intent = intent
        
        return best_intent, best_score
    
    def extract_entities(self, text: str) -> Dict[str, Any]:
        """Tr√≠ch xu·∫•t entities t·ª´ text"""
        entities = {}
        normalized_text = self.normalize_text(text)
        
        # Extract brand
        brands = ['nike', 'adidas', 'puma', 'vans', 'converse']
        for brand in brands:
            if brand in normalized_text:
                entities['brand'] = brand.capitalize()
                break
        
        # Extract gender
        if 'nam' in normalized_text and 'n·ªØ' not in normalized_text:
            entities['gender'] = 'Nam'
        elif 'n·ªØ' in normalized_text or 'nu' in normalized_text:
            entities['gender'] = 'N·ªØ'
        elif 'unisex' in normalized_text:
            entities['gender'] = 'Unisex'
        
        # Extract size
        size_patterns = [
            r'size\s*(\d{2})',
            r'(\d{2})\s*(size|s·ªë)',
            r's·ªë\s*(\d{2})'
        ]
        for pattern in size_patterns:
            match = re.search(pattern, normalized_text)
            if match:
                entities['size'] = match.group(1)
                break
        
        # Extract color
        colors = ['ƒëen', 'tr·∫Øng', 'ƒë·ªè', 'xanh', 'v√†ng', 'n√¢u', 'h·ªìng', 'x√°m', 'cam', 't√≠m']
        for color in colors:
            if color in normalized_text:
                entities['color'] = color
                break
        
        # Extract price range
        price_patterns = [
            r'd∆∞·ªõi\s*(\d+)\s*(tri·ªáu|tr|k|vnd)',
            r'(\d+)\s*(tri·ªáu|tr|k|vnd)\s*tr·ªü xu·ªëng',
            r'√≠t h∆°n\s*(\d+)\s*(tri·ªáu|tr|k|vnd)'
        ]
        for pattern in price_patterns:
            match = re.search(pattern, normalized_text)
            if match:
                amount = int(match.group(1))
                unit = match.group(2)
                if 'tri·ªáu' in unit or 'tr' in unit:
                    entities['max_price'] = amount * 1000000
                elif 'k' in unit:
                    entities['max_price'] = amount * 1000
                elif 'vnd' in unit:
                    entities['max_price'] = amount
                break
        
        return entities


class SentimentAnalyzer:
    """Ph√¢n t√≠ch c·∫£m x√∫c ng∆∞·ªùi d√πng n√¢ng cao"""
    
    def __init__(self):
        # T·ª´ kh√≥a c·∫£m x√∫c ti·∫øng Vi·ªát
        self.positive_words = [
            't·ªët', 'ƒë·∫πp', 'hay', 'tuy·ªát', 'ok', 'ƒë∆∞·ª£c', 'th√≠ch', 'y√™u', 'h√†i l√≤ng',
            'vui', 'h·∫°nh ph√∫c', 'th√∫ v·ªã', 'th√≠ch th√∫', 'c·∫£m ∆°n', 'thanks', 'perfect',
            'awesome', 'great', 'excellent', 'amazing', 'wonderful'
        ]
        self.negative_words = [
            't·ªá', 'x·∫•u', 'd·ªü', 'kh√¥ng th√≠ch', 'gh√©t', 'b·ª±c', 't·ª©c', 'kh√≥ ch·ªãu',
            'kh√¥ng h√†i l√≤ng', 'th·∫•t v·ªçng', 'bu·ªìn', 'lo l·∫Øng', 'kh√¥ng ok', 'bad',
            'terrible', 'awful', 'horrible', 'disappointed', 'angry'
        ]
        self.urgent_words = [
            'g·∫•p', 'kh·∫©n c·∫•p', 'nhanh', 'ngay', 'l·∫≠p t·ª©c', 'urgent', 'asap',
            'immediately', 'quickly', 'fast', 'hurry'
        ]
    
    def analyze_sentiment(self, message: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch c·∫£m x√∫c t·ª´ tin nh·∫Øn"""
        message_lower = message.lower()
        
        # ƒê·∫øm t·ª´ t√≠ch c·ª±c v√† ti√™u c·ª±c
        positive_count = sum(1 for word in self.positive_words if word in message_lower)
        negative_count = sum(1 for word in self.negative_words if word in message_lower)
        urgent_count = sum(1 for word in self.urgent_words if word in message_lower)
        
        # X√°c ƒë·ªãnh sentiment
        if positive_count > negative_count:
            sentiment = 'positive'
        elif negative_count > positive_count:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        # X√°c ƒë·ªãnh urgency
        is_urgent = urgent_count > 0 or '!' in message or '?' in message
        
        return {
            'sentiment': sentiment,
            'confidence': max(positive_count, negative_count) / len(message.split()),
            'is_urgent': is_urgent,
            'positive_words': positive_count,
            'negative_words': negative_count
        }


class FootyAI:
    """AI Shopping Assistant "Footy" cho FootFashion - Advanced Version"""
    
    def __init__(self):
        self.model = None
        self.memory = ConversationMemory()
        self.sentiment_analyzer = SentimentAnalyzer()
        self.nlp_processor = AdvancedNLPProcessor()
        self.context_builder = ProductContextBuilder()
        
        # Initialize Gemini model with proper API key
        self._initialize_gemini_model()
    
    def _initialize_gemini_model(self):
        """Initialize Gemini model with proper API key"""
        try:
            # Load API key directly from .env file
            from pathlib import Path
            from dotenv import load_dotenv
            
            # Get project root directory
            project_root = Path(__file__).resolve().parent.parent.parent.parent
            env_path = project_root / '.env'
            
            # Load .env file
            load_dotenv(env_path)
            
            # Get API key
            api_key = os.getenv('GEMINI_API_KEY')
            
            if api_key:
                genai.configure(api_key=api_key)
                # S·ª≠ d·ª•ng Gemini 2.5 Pro - Model m·ªõi nh·∫•t
                self.model = genai.GenerativeModel('gemini-2.5-pro')
                logger.info("Gemini 2.5 Pro model initialized successfully")
            else:
                logger.warning("GEMINI_API_KEY not found, Gemini model not initialized")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini model: {e}")
            # Fallback to older model if 2.5 Pro is not available
            try:
                if api_key:
                    genai.configure(api_key=api_key)
                    self.model = genai.GenerativeModel('gemini-1.5-pro-latest')
                    logger.info("Fallback to Gemini 1.5 Pro Latest model")
            except Exception as fallback_error:
                logger.error(f"Fallback model also failed: {fallback_error}")
    
    def detect_intent(self, message: str, context: List[Dict] = None) -> Tuple[str, float]:
        """
        Nh·∫≠n di·ªán √Ω ƒë·ªãnh ng∆∞·ªùi d√πng v·ªõi fuzzy matching v√† confidence score
        Enhanced context awareness for follow-up questions
        Returns: (intent, confidence_score)
        """
        message_lower = message.lower()
        
        # Enhanced context analysis for follow-up questions
        if context and len(context) > 0:
            last_conversation = context[-1]
            last_intent = last_conversation.get('intent', '')
            last_message = last_conversation.get('message', '').lower()
            
            # Follow-up question patterns
            follow_up_patterns = {
                'product_search': [
                    # Brand/type follow-ups
                    (r'\b(c√≤n|c√≥|th√™m)\b.*\b(th∆∞∆°ng hi·ªáu|brand|h√£ng|nike|adidas|puma|vans|converse)\b', 0.9),
                    (r'\b(th∆∞∆°ng hi·ªáu|brand|h√£ng)\b.*\b(kh√°c|kh√°c kh√¥ng|n√†o kh√°c)\b', 0.9),
                    (r'\b(c√≤n|c√≥)\b.*\b(th∆∞∆°ng hi·ªáu|brand|h√£ng)\b.*\b(kh√°c|n√†o)\b', 0.9),
                    
                    # Size/color follow-ups
                    (r'\b(c√≥|c√≤n)\b.*\b(size|m√†u|color|m√†u s·∫Øc)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.8),
                    (r'\b(size|m√†u|color)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.8),
                    
                    # Price follow-ups
                    (r'\b(c√≥|c√≤n)\b.*\b(gi√°|price|r·∫ª|ƒë·∫Øt)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.8),
                    (r'\b(gi√°|price)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.8),
                    
                    # General follow-ups
                    (r'\b(c√≤n|c√≥)\b.*\b(ƒë√¥i|gi√†y|d√©p)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.7),
                    (r'\b(ƒë√¥i|gi√†y|d√©p)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.7),
                    (r'\b(c√≤n|c√≥)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.6),
                    
                    # Disappointment/confusion follow-ups
                    (r'\b(·ªßa|uhm|hmm|kh√¥ng c√≥|kh√¥ng)\b.*\b(ƒë√¥i|gi√†y|d√©p)\b.*\b(ph√π h·ª£p|t·ªët|ƒë·∫πp)\b', 0.8),
                    (r'\b(·ªßa|uhm|hmm)\b.*\b(kh√¥ng c√≥|kh√¥ng)\b.*\b(ƒë√¥i|gi√†y|d√©p)\b', 0.7),
                    (r'\b(kh√¥ng c√≥|kh√¥ng)\b.*\b(ƒë√¥i|gi√†y|d√©p)\b.*\b(ph√π h·ª£p|t·ªët|ƒë·∫πp)\b', 0.7),
                ],
                'promotion': [
                    (r'\b(c√≤n|c√≥)\b.*\b(khuy·∫øn m√£i|sale|discount|m√£|voucher)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.8),
                    (r'\b(khuy·∫øn m√£i|sale|discount)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.8),
                ],
                'order_status': [
                    (r'\b(c√≤n|c√≥)\b.*\b(ƒë∆°n h√†ng|order)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.8),
                    (r'\b(ƒë∆°n h√†ng|order)\b.*\b(kh√°c|n√†o|kh√°c kh√¥ng)\b', 0.8),
                ]
            }
            
            # Check for follow-up patterns based on last intent
            if last_intent in follow_up_patterns:
                for pattern, confidence in follow_up_patterns[last_intent]:
                    if re.search(pattern, message_lower):
                        return last_intent, confidence
            
            # Context-based intent inheritance for short messages
            if len(message.strip()) < 30:  # Short follow-up messages
                if last_intent == 'product_search' and any(word in message_lower for word in ['c√≤n', 'c√≥', 'kh√°c', 'n√†o', '·ªßa', 'kh√¥ng', 'size', 'gi√°', 'm√†u', 'th∆∞∆°ng hi·ªáu', 'brand', 'ƒë√¥i', 'gi√†y']):
                    return 'product_search', 0.8
                elif last_intent == 'promotion' and any(word in message_lower for word in ['c√≤n', 'c√≥', 'kh√°c', 'n√†o', 'khuy·∫øn m√£i', 'sale']):
                    return 'promotion', 0.8
                elif last_intent == 'order_status' and any(word in message_lower for word in ['c√≤n', 'c√≥', 'kh√°c', 'n√†o', 'ƒë∆°n h√†ng', 'order']):
                    return 'order_status', 0.8
                elif last_intent == 'order_change_request' and any(word in message_lower for word in ['ƒë·ªïi', 'sang', 'm√†u', 'size', 'tr·∫Øng', 'ƒëen', 'xanh', 'ƒë·ªè']):
                    return 'order_change_request', 0.8
        
        # Greeting detection - check first for better accuracy (but not for follow-up questions)
        if not context and (any(word in message_lower for word in ['hey', 'hello', 'hi', 'ch√†o', 'xin ch√†o', 'xin chao']) or len(message.strip()) < 3):
            return 'greeting', 0.9
        
        # S·ª≠ d·ª•ng NLP processor ƒë·ªÉ detect intent
        intent, confidence = self.nlp_processor.fuzzy_match(message, [])
        
        # N·∫øu confidence th·∫•p, th·ª≠ c√°c pattern ƒë∆°n gi·∫£n v√† keyword-based detection
        if confidence < 0.4:  # Lower threshold for better coverage
            # Order status detection
            if any(word in message_lower for word in ['ƒë∆°n h√†ng', 'order', 'c·ªßa t√¥i', 'c·ªßa m√¨nh', 'g·∫ßn ƒë√¢y', 'xem']):
                if any(word in message_lower for word in ['ƒë∆°n h√†ng', 'order']):
                    return 'order_status', 0.7
            
            # Promotion detection
            if any(word in message_lower for word in ['khuy·∫øn m√£i', 'sale', 'discount', 'gi·∫£m gi√°', 'm√£', 'coupon', 'voucher']):
                return 'promotion', 0.7
            
            # Product search detection
            elif any(word in message_lower for word in ['gi√†y', 'd√©p', 'shoe', 'sneaker', 'boot', 'sandal', 'mua', 't√¨m', 'c·∫ßn']):
                return 'product_search', 0.6
            
            # Recommendation detection
            elif any(word in message_lower for word in ['g·ª£i √Ω', 'suggest', 'recommend', 't·ªët', 'ƒë·∫πp', 'hay', 'n√™n']):
                return 'recommendation', 0.6
            
            # Help detection
            elif any(word in message_lower for word in ['gi√∫p', 'help', 'h∆∞·ªõng d·∫´n', 'l√†m sao', 'nh∆∞ th·∫ø n√†o', 'kh√¥ng bi·∫øt']):
                return 'help', 0.6
            
            # Order change request detection
            elif any(word in message_lower for word in ['ƒë·ªïi', 'thay ƒë·ªïi', 'change', 'swap', 'mu·ªën ƒë·ªïi']):
                return 'order_change_request', 0.7
        
        return intent, confidence
    
    def generate_intelligent_response(self, message: str, intent: str, context: List[Dict] = None) -> str:
        """T·∫°o ph·∫£n h·ªìi th√¥ng minh b·∫±ng Gemini Pro v·ªõi context t·ª´ database"""
        # Ensure model is initialized
        if not self.model:
            self._initialize_gemini_model()
        
        if not self.model:
            return self._get_fallback_response(intent, context)
        
        try:
            # X√¢y d·ª±ng context t·ª´ database (gi·∫£m s·ªë l∆∞·ª£ng ƒë·ªÉ tr√°nh quota)
            product_context = self.context_builder.get_products_context(20)
            promotion_context = self.context_builder.get_promotions_context()
            
            # X√¢y d·ª±ng conversation context
            conversation_context = ""
            if context and len(context) > 0:
                recent_messages = context[-3:]  # L·∫•y 3 tin nh·∫Øn g·∫ßn nh·∫•t
                for conv in recent_messages:
                    conversation_context += f"Kh√°ch: {conv['message']}\n"
                    conversation_context += f"Footy: {conv['response'][:100]}...\n"
            
            # T·∫°o prompt th√¥ng minh
            prompt = f"""B·∫°n l√† Footy ‚Äì nh√¢n vi√™n t∆∞ v·∫•n gi√†y d√©p t·∫°i c·ª≠a h√†ng FootFashion.

Nhi·ªám v·ª• c·ªßa b·∫°n: Tr·∫£ l·ªùi th√¢n thi·ªán, t·ª± nhi√™n, nh∆∞ con ng∆∞·ªùi th·∫≠t. S·ª≠ d·ª•ng ti·∫øng Vi·ªát t·ª± nhi√™n v·ªõi gi·ªçng ƒëi·ªáu Gen Z nh∆∞ng l·ªãch s·ª±.

D∆∞·ªõi ƒë√¢y l√† danh s√°ch s·∫£n ph·∫©m trong c·ª≠a h√†ng:
{product_context}

Khuy·∫øn m√£i hi·ªán t·∫°i:
{promotion_context}

Ng·ªØ c·∫£nh h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
{conversation_context}

Kh√°ch h√†ng h·ªèi: "{message}"

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n, c√≥ c·∫£m x√∫c
- C√≥ th·ªÉ d√πng emoji nh·∫π: üëü üòä üî• ‚ù§Ô∏è
- N·∫øu kh√°ch h·ªèi v·ªÅ gi√°, m√†u, th∆∞∆°ng hi·ªáu, h√£y tr√≠ch xu·∫•t t·ª´ context s·∫£n ph·∫©m
- N·∫øu kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p, h√£y n√≥i l·ªãch s·ª± v√† g·ª£i √Ω kh√°c
- N·∫øu kh√°ch h·ªèi so s√°nh 2 s·∫£n ph·∫©m, h√£y so s√°nh nhanh ∆∞u nh∆∞·ª£c ƒëi·ªÉm
- Lu√¥n gi·ªØ gi·ªçng ƒëi·ªáu nhi·ªát t√¨nh, th√¢n thi·ªán nh∆∞ nh√¢n vi√™n b√°n h√†ng th·∫≠t

H√£y tr·∫£ l·ªùi:"""

            # G·ªçi Gemini Pro API
            response = self.model.generate_content(prompt)
            
            if response and response.text:
                return response.text.strip()
            else:
                return self._get_fallback_response(intent, context)
                
        except Exception as e:
            logger.error(f"Gemini API error: {e}")
            # Check if it's a quota error
            if "quota" in str(e).lower() or "429" in str(e):
                logger.warning("Gemini API quota exceeded, using fallback response")
            return self._get_fallback_response(intent, context)
    
    def generate_ai_response(self, message: str, intent: str, context: List[Dict] = None, sentiment: Dict = None, confidence: float = 0.0) -> str:
        """Wrapper method ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈©"""
        return self.generate_intelligent_response(message, intent, context)
    
    def _get_fallback_response(self, intent: str, context: List[Dict] = None) -> str:
        """Ph·∫£n h·ªìi d·ª± ph√≤ng khi Gemini API l·ªói"""
        responses = {
            'greeting': "Xin ch√†o! T√¥i l√† Footy, tr·ª£ l√Ω mua s·∫Øm c·ªßa FootFashion! üëã\n\nT√¥i c√≥ th·ªÉ gi√∫p b·∫°n:\nüîç T√¨m ki·∫øm gi√†y d√©p\nüí° G·ª£i √Ω s·∫£n ph·∫©m\nüéâ Xem khuy·∫øn m√£i\nüì¶ Ki·ªÉm tra ƒë∆°n h√†ng\n\nB·∫°n c·∫ßn g√¨ nh√©?",
            'product_search': "Ok n√®! üëã Footy ƒë√¢y, tr·ª£ l√Ω b√°n h√†ng c·ªßa FootFashion! B·∫°n mu·ªën t√¨m ƒë√¥i gi√†y n√†o ph√π h·ª£p kh√¥ng? üòä",
            'recommendation': "Chu·∫©n lu√¥n, ƒë·ªÉ em g·ª£i √Ω li·ªÅn nha üëü Em s·∫Ω t√¨m nh·ªØng ƒë√¥i gi√†y ph√π h·ª£p nh·∫•t cho b·∫°n!",
            'promotion': "Em s·∫Ω ki·ªÉm tra khuy·∫øn m√£i hi·ªán t·∫°i cho b·∫°n nha! üéâ",
            'order_status': "Em s·∫Ω ki·ªÉm tra tr·∫°ng th√°i ƒë∆°n h√†ng c·ªßa b·∫°n nha! üì¶",
            'order_change_request': "Em s·∫Ω gi√∫p b·∫°n thay ƒë·ªïi ƒë∆°n h√†ng! B·∫°n mu·ªën ƒë·ªïi size, m√†u s·∫Øc hay g√¨ kh√°c? üîÑ",
            'help': "Ok n√®! Em ·ªü ƒë√¢y ƒë·ªÉ gi√∫p b·∫°n nha üÜò B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ gi√†y d√©p, khuy·∫øn m√£i, ho·∫∑c ƒë∆°n h√†ng!",
            'unknown': "Ui em ch∆∞a hi·ªÉu r√µ √Ω b·∫°n l·∫Øm üòÖ B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ gi√†y d√©p, khuy·∫øn m√£i, ho·∫∑c ƒë∆°n h√†ng nha! Em s·∫Ω c·ªë g·∫Øng hi·ªÉu h∆°n! üòä"
        }
        return responses.get(intent, responses['unknown'])
    
    def get_cached_response(self, message: str, intent: str) -> Optional[str]:
        """L·∫•y ph·∫£n h·ªìi t·ª´ cache"""
        cache_key = f"footy_response_{hash(message)}_{intent}"
        return cache.get(cache_key)
    
    def cache_response(self, message: str, intent: str, response: str):
        """L∆∞u ph·∫£n h·ªìi v√†o cache"""
        cache_key = f"footy_response_{hash(message)}_{intent}"
        cache.set(cache_key, response, 3600)  # Cache 1 gi·ªù
    
    
    def process_message(self, message: str, user_id: str = None, session_id: str = None) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω tin nh·∫Øn ch√≠nh c·ªßa chatbot v·ªõi advanced features
        """
        start_time = time.time()
        
        # Validate input
        if not message or not message.strip():
            return {
                "type": "message",
                "content": "Xin l·ªói, t√¥i kh√¥ng hi·ªÉu. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ gi√†y d√©p, khuy·∫øn m√£i, ho·∫∑c ƒë∆°n h√†ng nh√©! üòä",
                "intent": "unknown",
                "confidence": 0.0,
                "sentiment": {"sentiment": "neutral", "confidence": 0.0},
                "processing_time": 0.0,
                "timestamp": timezone.now().isoformat()
            }
        
        # L·∫•y ng·ªØ c·∫£nh h·ªôi tho·∫°i
        context = self.memory.get_context(user_id or session_id) if (user_id or session_id) else []
        
        # Ph√¢n t√≠ch c·∫£m x√∫c
        sentiment = self.sentiment_analyzer.analyze_sentiment(message)
        
        # Nh·∫≠n di·ªán √Ω ƒë·ªãnh v·ªõi confidence score
        intent, confidence = self.detect_intent(message, context)
        
        # Ki·ªÉm tra cache tr∆∞·ªõc
        cached_response = self.get_cached_response(message, intent)
        if cached_response:
            ai_response = cached_response
        else:
            # T·∫°o ph·∫£n h·ªìi AI v·ªõi confidence
            ai_response = self.generate_ai_response(message, intent, context, sentiment, confidence)
            # L∆∞u v√†o cache
            self.cache_response(message, intent, ai_response)
        
        # T√≠nh th·ªùi gian x·ª≠ l√Ω
        processing_time = (time.time() - start_time) * 1000  # ms
        
        # Chu·∫©n b·ªã response data
        response_data = {
            "type": "message",
            "content": ai_response,
            "intent": intent,
            "confidence": confidence,
            "sentiment": sentiment,
            "processing_time": processing_time,
            "timestamp": timezone.now().isoformat()
        }
        
        # L∆∞u v√†o memory
        if user_id or session_id:
            self.memory.add_conversation(user_id or session_id, message, ai_response, intent)
        
        return response_data


# Global instance
footy_ai = FootyAI()
